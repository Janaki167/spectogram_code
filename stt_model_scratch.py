# -*- coding: utf-8 -*-
"""STT_model_scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-AMdQCzY-p5lny3Xcl2RxrWsKQRVBurQ
"""
import soundfile as sf
import pandas as pd
import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt
#the code below is to download the dataset, run it and comment rest of the code first
#check the file path of paraquet file and change in the code below when needed 
'''from huggingface_hub import hf_hub_download
from huggingface_hub import login

login()  
repo_id = "ai4bharat/indicvoices_r"  # Replace with the dataset repo name (e.g., "user/dataset")
filename = "Telugu/train-00001-of-00185.parquet"  # Adjust the filename

file_path = hf_hub_download(repo_id=repo_id, filename=filename, repo_type="dataset")
print("Downloaded file:", file_path)'''

# This will prompt you to enter your Hugging Face token


df = pd.read_parquet(r"C:\Users\janak\.cache\huggingface\hub\datasets--ai4bharat--indicvoices_r\snapshots\5f4495c91d500742a58d1be2ab07d77f73c0acf8\Telugu\train-00001-of-00185.parquet")
print(df.columns)
import os
def save_audio(row, output_dir="C://Users//janak//Downloads//audio_files"):
    
    os.makedirs(output_dir, exist_ok=True)

    audio_data = row["audio.bytes"]  # Extract raw bytes

    sampling_rate = 16000  # Get sample rate
    
    # Convert bytes to NumPy array
    audio_np = np.frombuffer(audio_data, dtype=np.int16)

    # Save as WAV file
    file_path = f"{output_dir}/audio_{row.name}.wav"
    sf.write(file_path, audio_np, sampling_rate)

# Write to another WAV file
    
    return file_path  # Return the path

# Apply function to save all audio files
df["audio_path"] = df.apply(save_audio, axis=1)
print(df["audio_path"].head())  # Check saved file paths

# Load a sample Parquet file
parquet_file = "/content/train-00000-of-00185.parquet"  # Change this based on your dataset


# Assuming the dataset has 'audio_path' and 'transcript' columns
sample_audio_path = df.loc[0, "audio_path"]
sample_transcript = df.loc[0, "text"]

print("Sample Audio Path:", sample_audio_path)
print("Sample Transcript:", sample_transcript)

# Load the actual audio file
waveform, sr = librosa.load(sample_audio_path, sr=16000)  # Resample to 16kHz

waveform, sr = librosa.load(sample_audio_path, sr=16000)  # Resample to 16kHz

n_fft = 400         # Number of FFT components
hop_length = 160    # Step size
n_mels = 80         # Number of Mel filterbanks

# Convert to Mel Spectrogram
mel_spec = librosa.feature.melspectrogram(
    y=waveform,
    sr=sr,
    n_fft=n_fft,
    hop_length=hop_length,
    n_mels=n_mels
)

# Apply log scaling
log_mel_spec = np.log(mel_spec + 1e-6)  # Avoid log(0) issue

# Plot Spectrogram
plt.figure(figsize=(10, 4))
librosa.display.specshow(log_mel_spec, sr=sr, hop_length=hop_length, y_axis="mel", x_axis="time")
plt.title("Log-Mel Spectrogram")
plt.colorbar(format="%+2.0f dB")
plt.show()

import torch
import torch.nn as nn

import torch
import torch.nn as nn

class ConvFeatureExtractor(nn.Module):
    def __init__(self, input_channels=80, conv_channels=256):  # Fix __init__
        super(ConvFeatureExtractor, self).__init__()
        self.conv1 = nn.Conv1d(input_channels, conv_channels, kernel_size=5, stride=2, padding=2)
        self.conv2 = nn.Conv1d(conv_channels, conv_channels, kernel_size=5, stride=2, padding=2)
        self.bn1 = nn.BatchNorm1d(conv_channels)
        self.bn2 = nn.BatchNorm1d(conv_channels)
        self.gelu = nn.GELU()  # Define GELU activation

    def forward(self, x):
        x = self.gelu(self.bn1(self.conv1(x)))
        x = self.gelu(self.bn2(self.conv2(x)))
        return x

import numpy as np
log_mel_spec = np.random.rand(100, 80)  # Example NumPy log-mel spectrogram

log_mel_spec_tensor = torch.tensor(log_mel_spec, dtype=torch.float32).unsqueeze(0)  # (1, 100, 80)
log_mel_spec_tensor = log_mel_spec_tensor.permute(0, 2, 1)  # (1, 80, 100)

conv_model = ConvFeatureExtractor()
conv_output = conv_model(log_mel_spec_tensor)
print(conv_output.shape)  # Should be (1, 256, new_time)


  # Expected output shape

 # (Batch, Time, Channels) â†’ (Batch, Channels, Time)

# Initialize model & run forward pass

conv_output = conv_model(log_mel_spec_tensor)

print("Input Shape:", log_mel_spec_tensor.shape)
print("Output Shape after Conv1D:", conv_output.shape)

print(conv_output)

